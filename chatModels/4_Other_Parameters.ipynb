{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "945aeb20-b276-4e1b-a6a3-75cd40724459",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MSI 1\\.conda\\envs\\oaivenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv,find_dotenv\n",
    "from langchain_core.prompts import PromptTemplate,ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage,HumanMessage,AIMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import SystemMessagePromptTemplate,HumanMessagePromptTemplate\n",
    "\n",
    "from langchain_community.callbacks.manager import get_openai_callback\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent, load_tools, initialize_agent,AgentType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5abca7dd-fcb7-4205-a6b2-cc34bb9e5c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(dotenv_path=find_dotenv(filename=\"D:\\LLM Courses\\Master Langchain Udemy\\.env\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30c373c1-07e2-42da-b84f-c26f5571655f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llmGemini=ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\").bind(logprobs=True)\n",
    "llmOpenAI=ChatOpenAI(model=\"gpt-3.5-turbo\").bind(logprobs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6308eff9-a200-44eb-8a61-0ebe7cca0114",
   "metadata": {},
   "source": [
    "<h3>Get Log Probabilities</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "143c3cc2-9021-43b6-9671-5c744742eb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does not work for Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5d92a47-199c-415f-ae37-62ca82d2178b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "GenerativeServiceClient.generate_content() got an unexpected keyword argument 'logprobs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m msg\u001b[38;5;241m=\u001b[39m\u001b[43mllmGemini\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhuman\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhow are you today?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\MSI 1\\.conda\\envs\\oaivenv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5354\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m   5349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5350\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   5351\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   5352\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   5353\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[1;32m-> 5354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5355\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5356\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5357\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5358\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\MSI 1\\.conda\\envs\\oaivenv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    285\u001b[0m         ChatGeneration,\n\u001b[1;32m--> 286\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mc:\\Users\\MSI 1\\.conda\\envs\\oaivenv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    780\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    785\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\MSI 1\\.conda\\envs\\oaivenv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[0;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[0;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    647\u001b[0m ]\n\u001b[0;32m    648\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[1;32mc:\\Users\\MSI 1\\.conda\\envs\\oaivenv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 633\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    638\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    639\u001b[0m         )\n\u001b[0;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\MSI 1\\.conda\\envs\\oaivenv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 851\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\MSI 1\\.conda\\envs\\oaivenv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:990\u001b[0m, in \u001b[0;36mChatGoogleGenerativeAI._generate\u001b[1;34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, **kwargs)\u001b[0m\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[0;32m    969\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    970\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    979\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    980\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[0;32m    981\u001b[0m     request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_request(\n\u001b[0;32m    982\u001b[0m         messages,\n\u001b[0;32m    983\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    988\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m    989\u001b[0m     )\n\u001b[1;32m--> 990\u001b[0m     response: GenerateContentResponse \u001b[38;5;241m=\u001b[39m \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    991\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    992\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    993\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    994\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    995\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "File \u001b[1;32mc:\\Users\\MSI 1\\.conda\\envs\\oaivenv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:208\u001b[0m, in \u001b[0;36m_chat_with_retry\u001b[1;34m(generation_method, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    206\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m--> 208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\MSI 1\\.conda\\envs\\oaivenv\\Lib\\site-packages\\tenacity\\__init__.py:330\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(\n\u001b[0;32m    327\u001b[0m     f, functools\u001b[38;5;241m.\u001b[39mWRAPPER_ASSIGNMENTS \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__defaults__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__kwdefaults__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    328\u001b[0m )\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[1;32m--> 330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\MSI 1\\.conda\\envs\\oaivenv\\Lib\\site-packages\\tenacity\\__init__.py:467\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    465\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 467\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\MSI 1\\.conda\\envs\\oaivenv\\Lib\\site-packages\\tenacity\\__init__.py:368\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    366\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[1;32m--> 368\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\MSI 1\\.conda\\envs\\oaivenv\\Lib\\site-packages\\tenacity\\__init__.py:390\u001b[0m, in \u001b[0;36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[1;34m(rs)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetryCallState\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mis_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mretry_run_result):\n\u001b[1;32m--> 390\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutcome\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    391\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\MSI 1\\.conda\\envs\\oaivenv\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Users\\MSI 1\\.conda\\envs\\oaivenv\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MSI 1\\.conda\\envs\\oaivenv\\Lib\\site-packages\\tenacity\\__init__.py:470\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 470\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[0;32m    472\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MSI 1\\.conda\\envs\\oaivenv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:206\u001b[0m, in \u001b[0;36m_chat_with_retry.<locals>._chat_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ChatGoogleGenerativeAIError(\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid argument provided to Gemini: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    204\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Users\\MSI 1\\.conda\\envs\\oaivenv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:190\u001b[0m, in \u001b[0;36m_chat_with_retry.<locals>._chat_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chat_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgeneration_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;66;03m# Do not retry for these errors.\u001b[39;00m\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mFailedPrecondition \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[1;31mTypeError\u001b[0m: GenerativeServiceClient.generate_content() got an unexpected keyword argument 'logprobs'"
     ]
    }
   ],
   "source": [
    "msg=llmGemini.invoke(input=(\"human\",\"how are you today?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a82acb8-7d83-435f-a86e-871343a3e2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Works for OpenAI Models\n",
    "msg=llmOpenAI.invoke(input=(\"human\",\"how are you today?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f555af9-8262-49be-bbb2-31186ea50577",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 23,\n",
       "  'prompt_tokens': 17,\n",
       "  'total_tokens': 40,\n",
       "  'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0},\n",
       "  'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}},\n",
       " 'model_name': 'gpt-3.5-turbo-0125',\n",
       " 'system_fingerprint': None,\n",
       " 'finish_reason': 'stop',\n",
       " 'logprobs': {'content': [{'token': 'I',\n",
       "    'bytes': [73],\n",
       "    'logprob': -0.2137053,\n",
       "    'top_logprobs': []},\n",
       "   {'token': \"'m\",\n",
       "    'bytes': [39, 109],\n",
       "    'logprob': -0.33866894,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' just',\n",
       "    'bytes': [32, 106, 117, 115, 116],\n",
       "    'logprob': -0.24022645,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' a',\n",
       "    'bytes': [32, 97],\n",
       "    'logprob': -0.0014491602,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' computer',\n",
       "    'bytes': [32, 99, 111, 109, 112, 117, 116, 101, 114],\n",
       "    'logprob': -0.05092288,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' program',\n",
       "    'bytes': [32, 112, 114, 111, 103, 114, 97, 109],\n",
       "    'logprob': -4.7875077e-05,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ',', 'bytes': [44], 'logprob': -0.07741755, 'top_logprobs': []},\n",
       "   {'token': ' so',\n",
       "    'bytes': [32, 115, 111],\n",
       "    'logprob': -0.0013470192,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' I',\n",
       "    'bytes': [32, 73],\n",
       "    'logprob': -3.5716304e-05,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' don',\n",
       "    'bytes': [32, 100, 111, 110],\n",
       "    'logprob': -0.0013567866,\n",
       "    'top_logprobs': []},\n",
       "   {'token': \"'t\",\n",
       "    'bytes': [39, 116],\n",
       "    'logprob': -1.2664457e-06,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' have',\n",
       "    'bytes': [32, 104, 97, 118, 101],\n",
       "    'logprob': -0.0035593275,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' feelings',\n",
       "    'bytes': [32, 102, 101, 101, 108, 105, 110, 103, 115],\n",
       "    'logprob': -0.022313854,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' or',\n",
       "    'bytes': [32, 111, 114],\n",
       "    'logprob': -0.1271617,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' emotions',\n",
       "    'bytes': [32, 101, 109, 111, 116, 105, 111, 110, 115],\n",
       "    'logprob': -0.016021464,\n",
       "    'top_logprobs': []},\n",
       "   {'token': '.', 'bytes': [46], 'logprob': -0.6350637, 'top_logprobs': []},\n",
       "   {'token': ' How',\n",
       "    'bytes': [32, 72, 111, 119],\n",
       "    'logprob': -1.264161,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' can',\n",
       "    'bytes': [32, 99, 97, 110],\n",
       "    'logprob': -0.0008391655,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' I',\n",
       "    'bytes': [32, 73],\n",
       "    'logprob': -4.1273333e-06,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' assist',\n",
       "    'bytes': [32, 97, 115, 115, 105, 115, 116],\n",
       "    'logprob': -0.004525062,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' you',\n",
       "    'bytes': [32, 121, 111, 117],\n",
       "    'logprob': -1.9862217e-05,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' today',\n",
       "    'bytes': [32, 116, 111, 100, 97, 121],\n",
       "    'logprob': -0.0005956857,\n",
       "    'top_logprobs': []},\n",
       "   {'token': '?',\n",
       "    'bytes': [63],\n",
       "    'logprob': -7.243103e-05,\n",
       "    'top_logprobs': []}],\n",
       "  'refusal': None}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg.response_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bf60995-f10c-4509-8d5a-df3ed45a825b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'token': 'I', 'bytes': [73], 'logprob': -0.2137053, 'top_logprobs': []},\n",
       " {'token': \"'m\",\n",
       "  'bytes': [39, 109],\n",
       "  'logprob': -0.33866894,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' just',\n",
       "  'bytes': [32, 106, 117, 115, 116],\n",
       "  'logprob': -0.24022645,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' a',\n",
       "  'bytes': [32, 97],\n",
       "  'logprob': -0.0014491602,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' computer',\n",
       "  'bytes': [32, 99, 111, 109, 112, 117, 116, 101, 114],\n",
       "  'logprob': -0.05092288,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' program',\n",
       "  'bytes': [32, 112, 114, 111, 103, 114, 97, 109],\n",
       "  'logprob': -4.7875077e-05,\n",
       "  'top_logprobs': []},\n",
       " {'token': ',', 'bytes': [44], 'logprob': -0.07741755, 'top_logprobs': []},\n",
       " {'token': ' so',\n",
       "  'bytes': [32, 115, 111],\n",
       "  'logprob': -0.0013470192,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' I',\n",
       "  'bytes': [32, 73],\n",
       "  'logprob': -3.5716304e-05,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' don',\n",
       "  'bytes': [32, 100, 111, 110],\n",
       "  'logprob': -0.0013567866,\n",
       "  'top_logprobs': []},\n",
       " {'token': \"'t\",\n",
       "  'bytes': [39, 116],\n",
       "  'logprob': -1.2664457e-06,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' have',\n",
       "  'bytes': [32, 104, 97, 118, 101],\n",
       "  'logprob': -0.0035593275,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' feelings',\n",
       "  'bytes': [32, 102, 101, 101, 108, 105, 110, 103, 115],\n",
       "  'logprob': -0.022313854,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' or',\n",
       "  'bytes': [32, 111, 114],\n",
       "  'logprob': -0.1271617,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' emotions',\n",
       "  'bytes': [32, 101, 109, 111, 116, 105, 111, 110, 115],\n",
       "  'logprob': -0.016021464,\n",
       "  'top_logprobs': []},\n",
       " {'token': '.', 'bytes': [46], 'logprob': -0.6350637, 'top_logprobs': []},\n",
       " {'token': ' How',\n",
       "  'bytes': [32, 72, 111, 119],\n",
       "  'logprob': -1.264161,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' can',\n",
       "  'bytes': [32, 99, 97, 110],\n",
       "  'logprob': -0.0008391655,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' I',\n",
       "  'bytes': [32, 73],\n",
       "  'logprob': -4.1273333e-06,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' assist',\n",
       "  'bytes': [32, 97, 115, 115, 105, 115, 116],\n",
       "  'logprob': -0.004525062,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' you',\n",
       "  'bytes': [32, 121, 111, 117],\n",
       "  'logprob': -1.9862217e-05,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' today',\n",
       "  'bytes': [32, 116, 111, 100, 97, 121],\n",
       "  'logprob': -0.0005956857,\n",
       "  'top_logprobs': []},\n",
       " {'token': '?', 'bytes': [63], 'logprob': -7.243103e-05, 'top_logprobs': []}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg.response_metadata[\"logprobs\"]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8a157fc-fb6f-4485-97a5-a36f19fe6f7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm just a computer program, so I don't have feelings or emotions. How can I assist you today?\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abfbdb4-d6d5-42ab-b4cb-cf768e577734",
   "metadata": {},
   "source": [
    "<h3>LogProbs Also provides Token Counts</h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efe59727-a791-47f7-93ee-3a56fa175c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llmGemini=ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "llmOpenAI=ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb4e9be7-e342-4915-8e2c-565598bb74ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg=llmOpenAI.invoke(input=(\"human\",\"how are you today?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50436428-d8cc-48be-875b-6fcf74f7b875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'completion_tokens': 40,\n",
       " 'prompt_tokens': 17,\n",
       " 'total_tokens': 57,\n",
       " 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0},\n",
       " 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg.response_metadata['token_usage']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c68609-dcfb-4ff9-b674-8b2d5896f456",
   "metadata": {},
   "source": [
    "<h3>Tracking Token Level Information</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8de59993-12d9-418f-9b63-5d2e5402b6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 21\n",
      "\tPrompt Tokens: 5\n",
      "\tCompletion Tokens: 16\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    result=llmGemini.invoke(input=\"Tell me a joke\")\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55c57a1b-2b24-49d3-9239-cce5d39b283d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 48\n",
      "\tPrompt Tokens: 16\n",
      "\tCompletion Tokens: 32\n",
      "Successful Requests: 2\n",
      "Total Cost (USD): $3.1e-05\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    result1=llmGemini.invoke(input=\"Tell me a joke\")\n",
    "    result2=llmOpenAI.invoke(input=\"Tell me a joke\")\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6351c598-3186-45b1-9b9e-60dfb7e4cf78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    result1=llmGemini.invoke(input=\"Tell me a joke\")\n",
    "    result2=llmOpenAI.invoke(input=\"Tell me a joke\")\n",
    "    print(cb.total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1e45d92-68a8-42fe-9ae9-90199fa9a2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You're a helpful assistant\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"placeholder\",\"{agent_scratchpad}\")\n",
    "    ]\n",
    ")\n",
    "tools=load_tools(tool_names=['wikipedia'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "808a9333-569a-4b72-b615-9b7736cb34e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Key 'title' is not supported in schema, ignoring\n",
      "Key 'title' is not supported in schema, ignoring\n"
     ]
    }
   ],
   "source": [
    "agent=create_tool_calling_agent(llm=llmGemini,tools=tools,prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e9ee230-1724-4a92-be42-370e8ff63bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "agentExecutor=AgentExecutor(agent=agent,verbose=True,stream_runnable=False,tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "010ec826-a122-42e2-9031-fad6c720d771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "{'name': 'wikipedia', 'description': 'A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.', 'parameters': {'type_': 6, 'description': 'Input for the WikipediaQuery tool.', 'properties': {'query': {'type_': 1, 'description': 'query to look up on wikipedia', 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `{'query': 'Hummingbird'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPage: Hummingbird\n",
      "Summary: Hummingbirds are birds native to the Americas and comprise the biological family Trochilidae. With approximately 366 species and 113 genera, they occur from Alaska to Tierra del Fuego, but most species are found in Central and South America. As of 2024, 21 hummingbird species are listed as endangered or critically endangered, with numerous species declining in population.\n",
      "Hummingbirds have varied specialized characteristics to enable rapid, maneuverable flight: exceptional metabolic capacity, adaptations to high altitude, sensitive visual and communication abilities, and long-distance migration in some species. Among all birds, male hummingbirds have the widest diversity of plumage color, particularly in blues, greens, and purples. Hummingbirds are the smallest mature birds, measuring 7.5–13 cm (3–5 in) in length. The smallest is the 5 cm (2.0 in) bee hummingbird, which weighs less than 2.0 g (0.07 oz), and the largest is the 23 cm (9 in) giant hummingbird, weighing 18–24 grams (0.63–0.85 oz). Noted for long beaks, hummingbirds are specialized for feeding on flower nectar, but all species also consume small insects.\n",
      "They are known as hummingbirds because of the humming sound created by their beating wings, which flap at high frequencies audible to other birds and humans. They hover at rapid wing-flapping rates, which vary from around 12 beats per second in the largest species to 80 per second in small hummingbirds.\n",
      "Hummingbirds have the highest mass-specific metabolic rate of any homeothermic animal. To conserve energy when food is scarce and at night when not foraging, they can enter torpor, a state similar to hibernation, and slow their metabolic rate to 1⁄15 of its normal rate. While most hummingbirds do not migrate, the rufous hummingbird has one of the longest migrations among birds, traveling twice per year between Alaska and Mexico, a distance of about 3,900 miles (6,300 km).\n",
      "Hummingbirds split from their sister group, the swifts and treeswifts, around 42 million years ago. The oldest known fossil hummingbird is Eurotrochilus, from the Rupelian Stage of Early Oligocene Europe.\n",
      "\n",
      "Page: Bee hummingbird\n",
      "Summary: The bee hummingbird, zunzuncito or Helena hummingbird (Mellisuga helenae) is a species of hummingbird, native to the island of Cuba in the Caribbean. It is the smallest known bird. The bee hummingbird feeds on nectar of flowers and bugs found in Cuba.\n",
      "\n",
      "\n",
      "\n",
      "Page: Hummingbird (film)\n",
      "Summary: Hummingbird (released as Redemption in the United States) is a 2013 action crime  drama film written and directed by Steven Knight in his directorial debut. It stars Jason Statham as a haunted, alcoholic veteran who befriends a Catholic nun, becomes involved in organized crime, and exacts vengeance on a man who beats and kills prostitutes.\n",
      "\n",
      "\u001b[0m{'name': 'wikipedia', 'description': 'A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.', 'parameters': {'type_': 6, 'description': 'Input for the WikipediaQuery tool.', 'properties': {'query': {'type_': 1, 'description': 'query to look up on wikipedia', 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['query'], 'format_': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
      "\u001b[32;1m\u001b[1;3mThe scientific name for a hummingbird is Trochilidae. I don't have information about the fastest bird species. \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Total Tokens: 914\n",
      "Prompt Tokens: 877\n",
      "Completion Tokens: 37\n",
      "Total Cost: 0.0\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    response=agentExecutor.invoke(input=\n",
    "                                  {\"input\":\"What's a hummingbird's scientific name and what's the fastest bird species?\"}\n",
    "                                 )\n",
    "    print(f\"Total Tokens: {cb.total_tokens}\")\n",
    "    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "    print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "    print(f\"Total Cost: {cb.total_cost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da6e5e0e-bd4b-49db-8875-901f5218ac18",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=load_tools(tool_names=[\"serpapi\",\"llm-math\"],llm=llmGemini)  # with wikipedia, it does not give the details\n",
    "agent=create_tool_calling_agent(llm=llmGemini,tools=tools,prompt=prompt)\n",
    "agentExecutor=AgentExecutor(agent=agent,verbose=True,stream_runnable=False,tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5520826c-c30c-453d-b624-6b1e39e8a8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "{'name': 'Search', 'description': 'A search engine. Useful for when you need to answer questions about current events. Input should be a search query.', 'parameters': {'type_': 6, 'properties': {'__arg1': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['__arg1'], 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
      "{'name': 'Calculator', 'description': 'Useful for when you need to answer questions about math.', 'parameters': {'type_': 6, 'properties': {'__arg1': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['__arg1'], 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Search` with `Hummingbird scientific name`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mHummingbirds belong to the order Apodiformes, meaning, \"unfooted birds.\" There are three families in this order: Trochilidae (hummingbirds), Hemiprocnidae (tree swifts), and Apodidae (swifts). The family Trochilidae belongs to its own Suborder, Trochili.\u001b[0m{'name': 'Search', 'description': 'A search engine. Useful for when you need to answer questions about current events. Input should be a search query.', 'parameters': {'type_': 6, 'properties': {'__arg1': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['__arg1'], 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
      "{'name': 'Calculator', 'description': 'Useful for when you need to answer questions about math.', 'parameters': {'type_': 6, 'properties': {'__arg1': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['__arg1'], 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Search` with `Fastest bird species`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mThe bird that can achieve the greatest airspeed is the peregrine falcon (Falco peregrinus), able to exceed 320 km/h (200 mph) in its dives.\u001b[0m{'name': 'Search', 'description': 'A search engine. Useful for when you need to answer questions about current events. Input should be a search query.', 'parameters': {'type_': 6, 'properties': {'__arg1': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['__arg1'], 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
      "{'name': 'Calculator', 'description': 'Useful for when you need to answer questions about math.', 'parameters': {'type_': 6, 'properties': {'__arg1': {'type_': 1, 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0', 'properties': {}, 'required': []}}, 'required': ['__arg1'], 'format_': '', 'description': '', 'nullable': False, 'enum': [], 'max_items': '0', 'min_items': '0'}}\n",
      "\u001b[32;1m\u001b[1;3mThe scientific name for hummingbirds is Trochilidae. The fastest bird species is the peregrine falcon, able to exceed 320 km/h (200 mph) in its dives. \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Total Tokens: 719\n",
      "Prompt Tokens: 645\n",
      "Completion Tokens: 74\n",
      "Total Cost: 0.0\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    response=agentExecutor.invoke(input=\n",
    "                                  {\"input\":\"What's a hummingbird's scientific name and what's the fastest bird species?\"}\n",
    "                                 )\n",
    "    print(f\"Total Tokens: {cb.total_tokens}\")\n",
    "    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "    print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "    print(f\"Total Cost: {cb.total_cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a89c89-03ec-49c3-ad7c-6f5008d568d4",
   "metadata": {},
   "source": [
    "<h3>Using an LLM for the same activity</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "715c4f09-40c5-451a-9405-9eab10512599",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=load_tools(tool_names=[\"serpapi\",\"llm-math\"],llm=llmGemini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d72f7584-cf19-4805-8137-1a9bc67332ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI 1\\AppData\\Local\\Temp\\ipykernel_8380\\1306811143.py:1: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 1.0. Use :meth:`~Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc.` instead.\n",
      "  agent=initialize_agent(tools=tools, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,verbose=True,llm=llmGemini)\n"
     ]
    }
   ],
   "source": [
    "agent=initialize_agent(tools=tools, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,verbose=True,llm=llmGemini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de54580c-8636-4569-8297-02ddf6279062",
   "metadata": {},
   "outputs": [],
   "source": [
    "agentExecutor=AgentExecutor(agent=agent,verbose=True,stream_runnable=False,tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5418ee14-b721-4a7e-9991-c3cdc0dc972f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to find out who Olivia Wilde's current boyfriend is and then calculate his age raised to the 0.23 power. \n",
      "Action: Search\n",
      "Action Input: Olivia Wilde boyfriend\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mIn January 2021, Wilde began dating singer Harry Styles after they met during the filming of Don't Worry Darling.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I need to find out Harry Styles' current age.\n",
      "Action: Search\n",
      "Action Input: Harry Styles age\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m{'type': 'organic_result', 'title': 'Harry Styles › Age ShareSend feedbackClose menu'}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I need to find out Harry Styles' current age.\n",
      "Action: Search\n",
      "Action Input: Harry Styles age\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m{'type': 'organic_result', 'title': 'Harry Styles › Age ShareSend feedbackClose menu'}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I need to find out Harry Styles' current age.\n",
      "Action: Search\n",
      "Action Input: Harry Styles age\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m{'type': 'organic_result', 'title': 'Harry Styles › Age ShareSend feedbackClose menu'}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I need to find out Harry Styles' current age.\n",
      "Action: Search\n",
      "Action Input: Harry Styles age\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m{'type': 'organic_result', 'title': 'Harry Styles › Age ShareSend feedbackClose menu'}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I need to find out who Olivia Wilde's current boyfriend is and then calculate his age raised to the 0.23 power. \n",
      "Action: Search\n",
      "Action Input: Olivia Wilde boyfriend\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mIn January 2021, Wilde began dating singer Harry Styles after they met during the filming of Don't Worry Darling.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I need to find out who Olivia Wilde's current boyfriend is and then calculate his age raised to the 0.23 power. \n",
      "Action: Search\n",
      "Action Input: Olivia Wilde boyfriend\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mIn January 2021, Wilde began dating singer Harry Styles after they met during the filming of Don't Worry Darling.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mQuestion: Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\n",
      "Thought: I need to find out who Olivia Wilde's current boyfriend is and then calculate his age raised to the 0.23 power. \n",
      "Action: Search\n",
      "Action Input: Olivia Wilde boyfriend\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mIn January 2021, Wilde began dating singer Harry Styles after they met during the filming of Don't Worry Darling.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mQuestion: Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\n",
      "Thought: I need to find out who Olivia Wilde's current boyfriend is and then calculate his age raised to the 0.23 power. \n",
      "Action: Search\n",
      "Action Input: Olivia Wilde boyfriend\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mIn January 2021, Wilde began dating singer Harry Styles after they met during the filming of Don't Worry Darling.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mQuestion: Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\n",
      "Thought: I need to find out who Olivia Wilde's current boyfriend is and then calculate his age raised to the 0.23 power. \n",
      "Action: Search\n",
      "Action Input: Olivia Wilde boyfriend\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mIn January 2021, Wilde began dating singer Harry Styles after they met during the filming of Don't Worry Darling.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mQuestion: Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\n",
      "Thought: I need to find out who Olivia Wilde's current boyfriend is and then calculate his age raised to the 0.23 power. \n",
      "Action: Search\n",
      "Action Input: Olivia Wilde boyfriend\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mIn January 2021, Wilde began dating singer Harry Styles after they met during the filming of Don't Worry Darling.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mQuestion: Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\n",
      "Thought: I need to find out who Olivia Wilde's current boyfriend is and then calculate his age raised to the 0.23 power. \n",
      "Action: Search\n",
      "Action Input: Olivia Wilde boyfriend\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mIn January 2021, Wilde began dating singer Harry Styles after they met during the filming of Don't Worry Darling.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mQuestion: Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\n",
      "Thought: I need to find out who Olivia Wilde's current boyfriend is and then calculate his age raised to the 0.23 power. \n",
      "Action: Search\n",
      "Action Input: Olivia Wilde boyfriend\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mIn January 2021, Wilde began dating singer Harry Styles after they met during the filming of Don't Worry Darling.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mQuestion: Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\n",
      "Thought: I need to find out who Olivia Wilde's current boyfriend is and then calculate his age raised to the 0.23 power. \n",
      "Action: Search\n",
      "Action Input: Olivia Wilde boyfriend\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mIn January 2021, Wilde began dating singer Harry Styles after they met during the filming of Don't Worry Darling.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mQuestion: Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\n",
      "Thought: I need to find out who Olivia Wilde's current boyfriend is and then calculate his age raised to the 0.23 power. \n",
      "Action: Search\n",
      "Action Input: Olivia Wilde boyfriend\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mIn January 2021, Wilde began dating singer Harry Styles after they met during the filming of Don't Worry Darling.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Total Tokens: 12421\n",
      "Prompt Tokens: 11655\n",
      "Completion Tokens: 766\n",
      "Total Cost: 0.0\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    response=agent.invoke(\n",
    "        input=\"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\"\n",
    "    )\n",
    "    print(f\"Total Tokens: {cb.total_tokens}\")\n",
    "    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "    print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "    print(f\"Total Cost: {cb.total_cost}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

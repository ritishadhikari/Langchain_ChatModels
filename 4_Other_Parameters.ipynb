{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "945aeb20-b276-4e1b-a6a3-75cd40724459",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv,find_dotenv\n",
    "from langchain_core.prompts import PromptTemplate,ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage,HumanMessage,AIMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import SystemMessagePromptTemplate,HumanMessagePromptTemplate\n",
    "\n",
    "from langchain_community.callbacks.manager import get_openai_callback\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent, load_tools, initialize_agent,AgentType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5abca7dd-fcb7-4205-a6b2-cc34bb9e5c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(dotenv_path=find_dotenv(filename=\"../.env\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30c373c1-07e2-42da-b84f-c26f5571655f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llmGemini=ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-001\").bind(logprobs=True)\n",
    "llmOpenAI=ChatOpenAI(model=\"gpt-4o-mini\").bind(logprobs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6308eff9-a200-44eb-8a61-0ebe7cca0114",
   "metadata": {},
   "source": [
    "<h3>Get Log Probabilities</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "143c3cc2-9021-43b6-9671-5c744742eb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does not work for Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5d92a47-199c-415f-ae37-62ca82d2178b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "GenerativeServiceClient.generate_content() got an unexpected keyword argument 'logprobs'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m msg=\u001b[43mllmGemini\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhuman\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhow are you today?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\lcnenv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5416\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5409\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5410\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5411\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5414\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   5415\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5417\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5418\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5419\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5420\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\lcnenv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:1175\u001b[39m, in \u001b[36mChatGoogleGenerativeAI.invoke\u001b[39m\u001b[34m(self, input, config, code_execution, stop, **kwargs)\u001b[39m\n\u001b[32m   1170\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1171\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1172\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mTools are already defined.\u001b[39m\u001b[33m\"\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mcode_execution tool can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt be defined\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1173\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1175\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\lcnenv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:368\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    356\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    358\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    363\u001b[39m     **kwargs: Any,\n\u001b[32m    364\u001b[39m ) -> BaseMessage:\n\u001b[32m    365\u001b[39m     config = ensure_config(config)\n\u001b[32m    366\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    367\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    378\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\lcnenv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:937\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    929\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    930\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    934\u001b[39m     **kwargs: Any,\n\u001b[32m    935\u001b[39m ) -> LLMResult:\n\u001b[32m    936\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m937\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\lcnenv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:759\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    756\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[32m    757\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    758\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    760\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    761\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    762\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    763\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    764\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    765\u001b[39m         )\n\u001b[32m    766\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    767\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\lcnenv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1002\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1000\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1001\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1002\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1006\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\lcnenv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:1242\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   1216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate\u001b[39m(\n\u001b[32m   1217\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1218\u001b[39m     messages: List[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1229\u001b[39m     **kwargs: Any,\n\u001b[32m   1230\u001b[39m ) -> ChatResult:\n\u001b[32m   1231\u001b[39m     request = \u001b[38;5;28mself\u001b[39m._prepare_request(\n\u001b[32m   1232\u001b[39m         messages,\n\u001b[32m   1233\u001b[39m         stop=stop,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1240\u001b[39m         tool_choice=tool_choice,\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     response: GenerateContentResponse = \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1244\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1245\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_method\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1246\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdefault_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1247\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1248\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\lcnenv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:208\u001b[39m, in \u001b[36m_chat_with_retry\u001b[39m\u001b[34m(generation_method, **kwargs)\u001b[39m\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    206\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\lcnenv\\Lib\\site-packages\\tenacity\\__init__.py:338\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    336\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    337\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\lcnenv\\Lib\\site-packages\\tenacity\\__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\lcnenv\\Lib\\site-packages\\tenacity\\__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\lcnenv\\Lib\\site-packages\\tenacity\\__init__.py:400\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[33m\"\u001b[39m\u001b[33mRetryCallState\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m         \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    401\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\lcnenv\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\lcnenv\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\lcnenv\\Lib\\site-packages\\tenacity\\__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\lcnenv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:206\u001b[39m, in \u001b[36m_chat_with_retry.<locals>._chat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ChatGoogleGenerativeAIError(\n\u001b[32m    203\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid argument provided to Gemini: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    204\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI\\anaconda3\\envs\\lcnenv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:190\u001b[39m, in \u001b[36m_chat_with_retry.<locals>._chat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chat_with_retry\u001b[39m(**kwargs: Any) -> Any:\n\u001b[32m    189\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgeneration_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m     \u001b[38;5;66;03m# Do not retry for these errors.\u001b[39;00m\n\u001b[32m    192\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m google.api_core.exceptions.FailedPrecondition \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[31mTypeError\u001b[39m: GenerativeServiceClient.generate_content() got an unexpected keyword argument 'logprobs'"
     ]
    }
   ],
   "source": [
    "msg=llmGemini.invoke(input=(\"human\",\"how are you today?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a82acb8-7d83-435f-a86e-871343a3e2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Works for OpenAI Models\n",
    "msg=llmOpenAI.invoke(input=(\"human\",\"how are you today?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f555af9-8262-49be-bbb2-31186ea50577",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 28,\n",
       "  'prompt_tokens': 17,\n",
       "  'total_tokens': 45,\n",
       "  'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "   'audio_tokens': 0,\n",
       "   'reasoning_tokens': 0,\n",
       "   'rejected_prediction_tokens': 0},\n",
       "  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
       " 'model_name': 'gpt-4o-mini-2024-07-18',\n",
       " 'system_fingerprint': 'fp_34a54ae93c',\n",
       " 'id': 'chatcmpl-BeJDK6lZlVI0dq8M0y7xjND5QpTax',\n",
       " 'finish_reason': 'stop',\n",
       " 'logprobs': {'content': [{'token': \"I'm\",\n",
       "    'bytes': [73, 39, 109],\n",
       "    'logprob': -0.04867241531610489,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' just',\n",
       "    'bytes': [32, 106, 117, 115, 116],\n",
       "    'logprob': -0.014044330455362797,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' a',\n",
       "    'bytes': [32, 97],\n",
       "    'logprob': -3.79811754100956e-05,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' computer',\n",
       "    'bytes': [32, 99, 111, 109, 112, 117, 116, 101, 114],\n",
       "    'logprob': -0.47782132029533386,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' program',\n",
       "    'bytes': [32, 112, 114, 111, 103, 114, 97, 109],\n",
       "    'logprob': -5.9795445849886164e-05,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ',',\n",
       "    'bytes': [44],\n",
       "    'logprob': -4.127333340875339e-06,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' so',\n",
       "    'bytes': [32, 115, 111],\n",
       "    'logprob': -0.07889000326395035,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' I',\n",
       "    'bytes': [32, 73],\n",
       "    'logprob': -1.9361264946837764e-07,\n",
       "    'top_logprobs': []},\n",
       "   {'token': \" don't\",\n",
       "    'bytes': [32, 100, 111, 110, 39, 116],\n",
       "    'logprob': -0.005234121344983578,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' have',\n",
       "    'bytes': [32, 104, 97, 118, 101],\n",
       "    'logprob': -0.0008142740116454661,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' feelings',\n",
       "    'bytes': [32, 102, 101, 101, 108, 105, 110, 103, 115],\n",
       "    'logprob': -1.0445127372804563e-05,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ',',\n",
       "    'bytes': [44],\n",
       "    'logprob': -0.002266113180667162,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' but',\n",
       "    'bytes': [32, 98, 117, 116],\n",
       "    'logprob': -3.128163257315464e-07,\n",
       "    'top_logprobs': []},\n",
       "   {'token': \" I'm\",\n",
       "    'bytes': [32, 73, 39, 109],\n",
       "    'logprob': -0.009456313215196133,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' here',\n",
       "    'bytes': [32, 104, 101, 114, 101],\n",
       "    'logprob': -0.0005535095697268844,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' and',\n",
       "    'bytes': [32, 97, 110, 100],\n",
       "    'logprob': -0.0008161775767803192,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' ready',\n",
       "    'bytes': [32, 114, 101, 97, 100, 121],\n",
       "    'logprob': -9.13388703338569e-06,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' to',\n",
       "    'bytes': [32, 116, 111],\n",
       "    'logprob': 0.0,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' help',\n",
       "    'bytes': [32, 104, 101, 108, 112],\n",
       "    'logprob': -0.03804149478673935,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' you',\n",
       "    'bytes': [32, 121, 111, 117],\n",
       "    'logprob': -0.001935661886818707,\n",
       "    'top_logprobs': []},\n",
       "   {'token': '!',\n",
       "    'bytes': [33],\n",
       "    'logprob': -0.030040310695767403,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' How',\n",
       "    'bytes': [32, 72, 111, 119],\n",
       "    'logprob': -2.176947600673884e-05,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' can',\n",
       "    'bytes': [32, 99, 97, 110],\n",
       "    'logprob': -0.05351966246962547,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' I', 'bytes': [32, 73], 'logprob': 0.0, 'top_logprobs': []},\n",
       "   {'token': ' assist',\n",
       "    'bytes': [32, 97, 115, 115, 105, 115, 116],\n",
       "    'logprob': -7.896309739408025e-07,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' you',\n",
       "    'bytes': [32, 121, 111, 117],\n",
       "    'logprob': 0.0,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' today',\n",
       "    'bytes': [32, 116, 111, 100, 97, 121],\n",
       "    'logprob': 0.0,\n",
       "    'top_logprobs': []},\n",
       "   {'token': '?', 'bytes': [63], 'logprob': 0.0, 'top_logprobs': []}],\n",
       "  'refusal': None}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg.response_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a54526",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bf60995-f10c-4509-8d5a-df3ed45a825b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'token': \"I'm\",\n",
       "  'bytes': [73, 39, 109],\n",
       "  'logprob': -0.04867241531610489,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' just',\n",
       "  'bytes': [32, 106, 117, 115, 116],\n",
       "  'logprob': -0.014044330455362797,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' a',\n",
       "  'bytes': [32, 97],\n",
       "  'logprob': -3.79811754100956e-05,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' computer',\n",
       "  'bytes': [32, 99, 111, 109, 112, 117, 116, 101, 114],\n",
       "  'logprob': -0.47782132029533386,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' program',\n",
       "  'bytes': [32, 112, 114, 111, 103, 114, 97, 109],\n",
       "  'logprob': -5.9795445849886164e-05,\n",
       "  'top_logprobs': []},\n",
       " {'token': ',',\n",
       "  'bytes': [44],\n",
       "  'logprob': -4.127333340875339e-06,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' so',\n",
       "  'bytes': [32, 115, 111],\n",
       "  'logprob': -0.07889000326395035,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' I',\n",
       "  'bytes': [32, 73],\n",
       "  'logprob': -1.9361264946837764e-07,\n",
       "  'top_logprobs': []},\n",
       " {'token': \" don't\",\n",
       "  'bytes': [32, 100, 111, 110, 39, 116],\n",
       "  'logprob': -0.005234121344983578,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' have',\n",
       "  'bytes': [32, 104, 97, 118, 101],\n",
       "  'logprob': -0.0008142740116454661,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' feelings',\n",
       "  'bytes': [32, 102, 101, 101, 108, 105, 110, 103, 115],\n",
       "  'logprob': -1.0445127372804563e-05,\n",
       "  'top_logprobs': []},\n",
       " {'token': ',',\n",
       "  'bytes': [44],\n",
       "  'logprob': -0.002266113180667162,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' but',\n",
       "  'bytes': [32, 98, 117, 116],\n",
       "  'logprob': -3.128163257315464e-07,\n",
       "  'top_logprobs': []},\n",
       " {'token': \" I'm\",\n",
       "  'bytes': [32, 73, 39, 109],\n",
       "  'logprob': -0.009456313215196133,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' here',\n",
       "  'bytes': [32, 104, 101, 114, 101],\n",
       "  'logprob': -0.0005535095697268844,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' and',\n",
       "  'bytes': [32, 97, 110, 100],\n",
       "  'logprob': -0.0008161775767803192,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' ready',\n",
       "  'bytes': [32, 114, 101, 97, 100, 121],\n",
       "  'logprob': -9.13388703338569e-06,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' to', 'bytes': [32, 116, 111], 'logprob': 0.0, 'top_logprobs': []},\n",
       " {'token': ' help',\n",
       "  'bytes': [32, 104, 101, 108, 112],\n",
       "  'logprob': -0.03804149478673935,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' you',\n",
       "  'bytes': [32, 121, 111, 117],\n",
       "  'logprob': -0.001935661886818707,\n",
       "  'top_logprobs': []},\n",
       " {'token': '!',\n",
       "  'bytes': [33],\n",
       "  'logprob': -0.030040310695767403,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' How',\n",
       "  'bytes': [32, 72, 111, 119],\n",
       "  'logprob': -2.176947600673884e-05,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' can',\n",
       "  'bytes': [32, 99, 97, 110],\n",
       "  'logprob': -0.05351966246962547,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' I', 'bytes': [32, 73], 'logprob': 0.0, 'top_logprobs': []},\n",
       " {'token': ' assist',\n",
       "  'bytes': [32, 97, 115, 115, 105, 115, 116],\n",
       "  'logprob': -7.896309739408025e-07,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' you',\n",
       "  'bytes': [32, 121, 111, 117],\n",
       "  'logprob': 0.0,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' today',\n",
       "  'bytes': [32, 116, 111, 100, 97, 121],\n",
       "  'logprob': 0.0,\n",
       "  'top_logprobs': []},\n",
       " {'token': '?', 'bytes': [63], 'logprob': 0.0, 'top_logprobs': []}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg.response_metadata[\"logprobs\"]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e55729e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'token': ' to', 'bytes': [32, 116, 111], 'logprob': 0.0, 'top_logprobs': []},\n",
       " {'token': ' I', 'bytes': [32, 73], 'logprob': 0.0, 'top_logprobs': []},\n",
       " {'token': ' you',\n",
       "  'bytes': [32, 121, 111, 117],\n",
       "  'logprob': 0.0,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' today',\n",
       "  'bytes': [32, 116, 111, 100, 97, 121],\n",
       "  'logprob': 0.0,\n",
       "  'top_logprobs': []},\n",
       " {'token': '?', 'bytes': [63], 'logprob': 0.0, 'top_logprobs': []},\n",
       " {'token': ' I',\n",
       "  'bytes': [32, 73],\n",
       "  'logprob': -1.9361264946837764e-07,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' but',\n",
       "  'bytes': [32, 98, 117, 116],\n",
       "  'logprob': -3.128163257315464e-07,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' assist',\n",
       "  'bytes': [32, 97, 115, 115, 105, 115, 116],\n",
       "  'logprob': -7.896309739408025e-07,\n",
       "  'top_logprobs': []},\n",
       " {'token': ',',\n",
       "  'bytes': [44],\n",
       "  'logprob': -4.127333340875339e-06,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' ready',\n",
       "  'bytes': [32, 114, 101, 97, 100, 121],\n",
       "  'logprob': -9.13388703338569e-06,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' feelings',\n",
       "  'bytes': [32, 102, 101, 101, 108, 105, 110, 103, 115],\n",
       "  'logprob': -1.0445127372804563e-05,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' How',\n",
       "  'bytes': [32, 72, 111, 119],\n",
       "  'logprob': -2.176947600673884e-05,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' a',\n",
       "  'bytes': [32, 97],\n",
       "  'logprob': -3.79811754100956e-05,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' program',\n",
       "  'bytes': [32, 112, 114, 111, 103, 114, 97, 109],\n",
       "  'logprob': -5.9795445849886164e-05,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' here',\n",
       "  'bytes': [32, 104, 101, 114, 101],\n",
       "  'logprob': -0.0005535095697268844,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' have',\n",
       "  'bytes': [32, 104, 97, 118, 101],\n",
       "  'logprob': -0.0008142740116454661,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' and',\n",
       "  'bytes': [32, 97, 110, 100],\n",
       "  'logprob': -0.0008161775767803192,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' you',\n",
       "  'bytes': [32, 121, 111, 117],\n",
       "  'logprob': -0.001935661886818707,\n",
       "  'top_logprobs': []},\n",
       " {'token': ',',\n",
       "  'bytes': [44],\n",
       "  'logprob': -0.002266113180667162,\n",
       "  'top_logprobs': []},\n",
       " {'token': \" don't\",\n",
       "  'bytes': [32, 100, 111, 110, 39, 116],\n",
       "  'logprob': -0.005234121344983578,\n",
       "  'top_logprobs': []},\n",
       " {'token': \" I'm\",\n",
       "  'bytes': [32, 73, 39, 109],\n",
       "  'logprob': -0.009456313215196133,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' just',\n",
       "  'bytes': [32, 106, 117, 115, 116],\n",
       "  'logprob': -0.014044330455362797,\n",
       "  'top_logprobs': []},\n",
       " {'token': '!',\n",
       "  'bytes': [33],\n",
       "  'logprob': -0.030040310695767403,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' help',\n",
       "  'bytes': [32, 104, 101, 108, 112],\n",
       "  'logprob': -0.03804149478673935,\n",
       "  'top_logprobs': []},\n",
       " {'token': \"I'm\",\n",
       "  'bytes': [73, 39, 109],\n",
       "  'logprob': -0.04867241531610489,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' can',\n",
       "  'bytes': [32, 99, 97, 110],\n",
       "  'logprob': -0.05351966246962547,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' so',\n",
       "  'bytes': [32, 115, 111],\n",
       "  'logprob': -0.07889000326395035,\n",
       "  'top_logprobs': []},\n",
       " {'token': ' computer',\n",
       "  'bytes': [32, 99, 111, 109, 112, 117, 116, 101, 114],\n",
       "  'logprob': -0.47782132029533386,\n",
       "  'top_logprobs': []}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(msg.response_metadata[\"logprobs\"]['content'],key=lambda s:s['logprob'],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8a157fc-fb6f-4485-97a5-a36f19fe6f7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm just a computer program, so I don't have feelings or emotions. How can I assist you today?\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abfbdb4-d6d5-42ab-b4cb-cf768e577734",
   "metadata": {},
   "source": [
    "<h3>LogProbs Also provides Token Counts</h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efe59727-a791-47f7-93ee-3a56fa175c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llmGemini=ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "llmOpenAI=ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb4e9be7-e342-4915-8e2c-565598bb74ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg=llmOpenAI.invoke(input=(\"human\",\"how are you today?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50436428-d8cc-48be-875b-6fcf74f7b875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'completion_tokens': 38,\n",
       " 'prompt_tokens': 17,\n",
       " 'total_tokens': 55,\n",
       " 'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "  'audio_tokens': 0,\n",
       "  'reasoning_tokens': 0,\n",
       "  'rejected_prediction_tokens': 0},\n",
       " 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg.response_metadata['token_usage']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c68609-dcfb-4ff9-b674-8b2d5896f456",
   "metadata": {},
   "source": [
    "<h3>Tracking Token Level Information</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8de59993-12d9-418f-9b63-5d2e5402b6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 21\n",
      "\tPrompt Tokens: 4\n",
      "\t\tPrompt Tokens Cached: 0\n",
      "\tCompletion Tokens: 17\n",
      "\t\tReasoning Tokens: 0\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    result=llmGemini.invoke(input=\"Tell me a joke\")\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55c57a1b-2b24-49d3-9239-cce5d39b283d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 49\n",
      "\tPrompt Tokens: 15\n",
      "\t\tPrompt Tokens Cached: 0\n",
      "\tCompletion Tokens: 34\n",
      "\t\tReasoning Tokens: 0\n",
      "Successful Requests: 2\n",
      "Total Cost (USD): $3.1e-05\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    result1=llmGemini.invoke(input=\"Tell me a joke\")\n",
    "    result2=llmOpenAI.invoke(input=\"Tell me a joke\")\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6351c598-3186-45b1-9b9e-60dfb7e4cf78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    result1=llmGemini.invoke(input=\"Tell me a joke\")\n",
    "    result2=llmOpenAI.invoke(input=\"Tell me a joke\")\n",
    "    print(cb.total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1e45d92-68a8-42fe-9ae9-90199fa9a2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You're a helpful assistant\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"placeholder\",\"{agent_scratchpad}\")\n",
    "    ]\n",
    ")\n",
    "tools=load_tools(tool_names=['wikipedia'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70c5bb8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from 'c:\\\\Users\\\\MSI\\\\anaconda3\\\\envs\\\\lcnenv\\\\Lib\\\\site-packages\\\\wikipedia\\\\__init__.py'>, top_k_results=3, lang='en', load_all_available_meta=False, doc_content_chars_max=4000))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "808a9333-569a-4b72-b615-9b7736cb34e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=create_tool_calling_agent(llm=llmGemini,tools=tools,prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e9ee230-1724-4a92-be42-370e8ff63bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "agentExecutor=AgentExecutor(agent=agent,verbose=True,stream_runnable=False,tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "010ec826-a122-42e2-9031-fad6c720d771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `{'query': 'Sachin Tendulkar'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPage: Sachin Tendulkar\n",
      "Summary: Sachin Ramesh Tendulkar ( ; Marathi: [sətɕin t̪eɳɖulkəɾ]; born 24 April 1973) is an Indian former international cricketer who captained the Indian national team. He is widely regarded as one of the greatest cricketers of all time, and is the holder of several world records, including being the all-time highest run-scorer in both ODI and Test cricket, receiving the most player of the match awards in international cricket, and being the only batsman to score 100 international centuries. Tendulkar was a Member of Parliament, Rajya Sabha by presidential nomination from 2012 to 2018. \n",
      "Tendulkar took up cricket at the age of eleven, made his Test match debut on 15 November 1989 against Pakistan in Karachi at the age of sixteen, and went on to represent Mumbai domestically and India internationally for over 24 years. In 2002, halfway through his career, Wisden ranked him the second-greatest Test batsman of all time, behind Don Bradman, and the second-greatest ODI batsman of all time, behind Viv Richards. The same year, Tendulkar was a part of the team that was one of the joint-winners of the 2002 ICC Champions Trophy. Later in his career, Tendulkar was part of the Indian team that won the 2011 Cricket World Cup, his first win in six World Cup appearances for India. He had previously been named \"Player of the Tournament\" at the 2003 World Cup.\n",
      "Tendulkar has received several awards from the government of India: the Arjuna Award (1994), the Khel Ratna Award (1997), the Padma Shri (1998), and the Padma Vibhushan (2008). After Tendulkar played his last match in November 2013, the Prime Minister's Office announced the decision to award him the Bharat Ratna, India's highest civilian award. He was the first sportsperson to receive the award and, as of 2024, is the youngest recipient. In 2010, Time included Tendulkar in its annual list of the most influential people in the world. Tendulkar was awarded the Sir Garfield Sobers Trophy for cricketer of the year at the 2010 International Cricket Council (ICC) Awards.\n",
      "Having retired from ODI cricket in 2012, he retired from all forms of cricket in November 2013 after playing his 200th Test match. Tendulkar played 664 international cricket matches in total, scoring 34,357 runs. In 2013, Tendulkar was included in an all-time Test World XI to mark the 150th anniversary of Wisden Cricketers' Almanack, and he was one of only two specialist batsmen of the post–World War II era, along with Viv Richards, to get featured in the team. In 2019, he was inducted into the ICC Cricket Hall of Fame.\n",
      "\n",
      "\n",
      "\n",
      "Page: Sachin: A Billion Dreams\n",
      "Summary: Sachin: A Billion Dreams is a 2017 Indian documentary sports film directed by James Erskine and produced by Ravi Bhagchandka and Shrikant Bhasi under the banners 200 NotOut Productions and Carnival Motion Pictures. The film is a documentary on the life of Indian cricketer Sachin Tendulkar. It captures Tendulkar's cricket and personal life in substantial detail, as well as reveals a few aspects of his life which have never been heard of or seen before.\n",
      "The film is simultaneously shot in Marathi, Hindi and English and was released on 26 May 2017, along with the dubbed versions in Tamil and Telugu. This film was declared tax free in Maharashtra, Chhattisgarh, Karnataka, Kerala, and Odisha. The film was an average grosser at the box office.\n",
      "\n",
      "\n",
      "\n",
      "Page: List of international cricket centuries by Sachin Tendulkar\n",
      "Summary: Sachin Tendulkar played for India, widely acknowledged as one of the greatest batsmen of all time, he is the most prolific run-scorer in international cricket. Tendulkar has scored the highest number of centuries (100 or more runs) in Test matches and One Day International (ODI) matches organised by the International Cricket Council. His total of 51 centuries in Test matches is a world record for highest number of centuries by a batsman and his 49 centuries in ODI matches are the second highest number of centuries after Virat Kohli. He \u001b[0m\u001b[32;1m\u001b[1;3mSachin Ramesh Tendulkar is a former Indian cricketer widely regarded as one of the greatest of all time.  He holds numerous world records, including most runs in both ODI and Test cricket, and the most player of the match awards.  He also scored 100 international centuries, a feat unmatched by any other batsman.  He played for over 24 years, representing Mumbai domestically and India internationally.  He received numerous awards including the Bharat Ratna, India's highest civilian award, and was inducted into the ICC Cricket Hall of Fame.  A documentary film, \"Sachin: A Billion Dreams,\" details his life and career.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Total Tokens: 1243\n",
      "Prompt Tokens: 1104\n",
      "Completion Tokens: 139\n",
      "Total Cost: 0.0\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    response=agentExecutor.invoke(input=\n",
    "                                  {\"input\":\"Sachin Tendulkar?\"}\n",
    "                                 )\n",
    "    print(f\"Total Tokens: {cb.total_tokens}\")\n",
    "    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "    print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "    print(f\"Total Cost: {cb.total_cost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da6e5e0e-bd4b-49db-8875-901f5218ac18",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=load_tools(tool_names=[\"serpapi\",\"llm-math\"],llm=llmGemini)  # with wikipedia, it does not give the details\n",
    "agent=create_tool_calling_agent(llm=llmGemini,tools=tools,prompt=prompt)\n",
    "agentExecutor=AgentExecutor(agent=agent,verbose=True,stream_runnable=False,tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5520826c-c30c-453d-b624-6b1e39e8a8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Search` with `What is the fastest bird species?`\n",
      "responded: I am able to answer the first question.  I will need to search for the answer to the second question.\n",
      "\n",
      "To answer your first question: I lack the specific knowledge to provide the scientific name of a hummingbird.  I would need access to a database of animal information to answer this question.\n",
      "\n",
      "To answer your second question: I will need to use a search engine. I will use the available `default_api`'s `Search` function.\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mthe Peregrine Falcon\u001b[0m\u001b[32;1m\u001b[1;3mI am not able to find the scientific name for a hummingbird, but the Peregrine Falcon is the fastest bird species.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Total Tokens: 299\n",
      "Prompt Tokens: 169\n",
      "Completion Tokens: 130\n",
      "Total Cost: 0.0\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    response=agentExecutor.invoke(input=\n",
    "                                  {\"input\":\"What's a hummingbird's scientific name and what's the fastest bird species?\"}\n",
    "                                 )\n",
    "    print(f\"Total Tokens: {cb.total_tokens}\")\n",
    "    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "    print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "    print(f\"Total Cost: {cb.total_cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a89c89-03ec-49c3-ad7c-6f5008d568d4",
   "metadata": {},
   "source": [
    "<h3>Using an LLM for the same activity</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "715c4f09-40c5-451a-9405-9eab10512599",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=load_tools(tool_names=[\"serpapi\",\"llm-math\"],llm=llmGemini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "19893894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tool(name='Search', description='A search engine. Useful for when you need to answer questions about current events. Input should be a search query.', func=<bound method SerpAPIWrapper.run of SerpAPIWrapper(search_engine=<class 'serpapi.google_search.GoogleSearch'>, params={'engine': 'google', 'google_domain': 'google.com', 'gl': 'us', 'hl': 'en'}, serpapi_api_key='7f1f1a7b3b4c6064292064e174bd3d699c952fb4d998a31aafdd92b8e16edd64', aiosession=None)>, coroutine=<bound method SerpAPIWrapper.arun of SerpAPIWrapper(search_engine=<class 'serpapi.google_search.GoogleSearch'>, params={'engine': 'google', 'google_domain': 'google.com', 'gl': 'us', 'hl': 'en'}, serpapi_api_key='7f1f1a7b3b4c6064292064e174bd3d699c952fb4d998a31aafdd92b8e16edd64', aiosession=None)>),\n",
       " Tool(name='Calculator', description='Useful for when you need to answer questions about math.', func=<bound method Chain.run of LLMMathChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=ChatGoogleGenerativeAI(model='models/gemini-1.5-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x0000021CD1840C90>, default_metadata=()), output_parser=StrOutputParser(), llm_kwargs={}))>, coroutine=<bound method Chain.arun of LLMMathChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=ChatGoogleGenerativeAI(model='models/gemini-1.5-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x0000021CD1840C90>, default_metadata=()), output_parser=StrOutputParser(), llm_kwargs={}))>)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d72f7584-cf19-4805-8137-1a9bc67332ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI\\AppData\\Local\\Temp\\ipykernel_15808\\1306811143.py:1: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent=initialize_agent(tools=tools, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,verbose=True,llm=llmGemini)\n"
     ]
    }
   ],
   "source": [
    "agent=initialize_agent(tools=tools, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,verbose=True,llm=llmGemini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "de54580c-8636-4569-8297-02ddf6279062",
   "metadata": {},
   "outputs": [],
   "source": [
    "agentExecutor=AgentExecutor(agent=agent,verbose=True,stream_runnable=False,tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5418ee14-b721-4a7e-9991-c3cdc0dc972f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to find out who Olivia Wilde's current boyfriend is and then calculate his age raised to the 0.23 power.  I'll use Search to find his name and age, and then Calculator to perform the calculation.\n",
      "\n",
      "Action: Search\n",
      "Action Input: \"Olivia Wilde boyfriend\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAs of early 2025, Wilde has been dating actor Dane DiLiegro.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I need to find Dane DiLiegro's age.\n",
      "Action: Search\n",
      "Action Input: \"Dane DiLiegro age\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m36 years\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I now have Dane DiLiegro's age. I need to calculate 36 to the power of 0.23.\n",
      "Action: Calculator\n",
      "Action Input: 36**0.23\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 2.2800773226742175\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: Olivia Wilde's boyfriend, Dane DiLiegro, is 36 years old.  36 raised to the power of 0.23 is approximately 2.28.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Total Tokens: 1998\n",
      "Prompt Tokens: 1778\n",
      "Completion Tokens: 220\n",
      "Total Cost: 0.0\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    response=agent.invoke(\n",
    "        input=\"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\"\n",
    "    )\n",
    "    print(f\"Total Tokens: {cb.total_tokens}\")\n",
    "    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "    print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "    print(f\"Total Cost: {cb.total_cost}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lcnenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
